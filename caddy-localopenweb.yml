services:
  caddy-proxy:
    image: caddy:latest
    container_name: ollama-caddy-localopenweb
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./caddy/Caddyfile-localopenweb:/etc/caddy/Caddyfile
      - ./caddy/data:/data
      - ./caddy/config:/config
    networks:
        - openwebui-bridge
    restart: unless-stopped

# equivalent:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-local
    ports:
      - "8080:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      # set open-webui to call ollama through a loadbalancer. this way we can have multiple instances of ollama
      # depending on if a server is down. (For example, I might want to keep my home PC turned off at night)
      # ollama is running on a private VPN tunnel
      - OLLAMA_BASE_URL=http://haproxy:11444 
    networks:
        - openwebui-bridge 
    restart: unless-stopped

  haproxy:
    image: haproxy:latest
    container_name: haproxy
    ports:
      - "8081:8080"  # Expose the load balancer on port 8081 (adjust as needed)
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    networks:
      - openwebui-bridge
    restart: unless-stopped


  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11333:11333"
    networks:
      - openwebui-bridge
    expose:
      - "11333"         # Expose port 11434 internally
    environment:
      # Configure local instance of ollama to run on 11333
      - OLLAMA_HOST=0.0.0.0:11333
    restart: unless-stopped


networks:
  openwebui-bridge:
    driver: bridge

volumes:
  caddy_data:
  caddy_config:
  open-webui:

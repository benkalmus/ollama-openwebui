services:
  # This exposes your OpenWebUI to the internet
  caddy-proxy:
    image: caddy:latest
    container_name: ollama-caddy-localopenweb
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile
      - caddy-data:/data
      - caddy-config:/config
    networks:
      - internal-network
    restart: unless-stopped

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    # image: ghcr.io/open-webui/open-webui:0.5.15   # tag an image with `docker tag <hash> ghcr.io/open-webui/open-webui:0.5.15`
    container_name: open-webui-local
    ports:
      - "8080:8080" # for debugging, you will be able to call localhost:8080 from your private network
    extra_hosts:
      - "host.docker.internal:host-gateway" # NOTE: Dont forget to allow port 11469 with ufw!!!
    volumes:
      - open-webui:/app/backend/data
    environment:
      # set open-webui to call ollama through a loadbalancer. this way we can have multiple instances of ollama
      # depending on if a server is down. (For example, I might want to keep my home PC turned off at night)
      # ollama is running on a private VPN tunnel
      - OLLAMA_BASE_URL=http://haproxy:11444
    networks:
      - internal-network
    restart: unless-stopped

  haproxy:
    image: haproxy:latest
    container_name: haproxy
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    networks:
      - internal-network
    restart: unless-stopped

  # Docker instance of ollama.
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama-docker:/root/.ollama
    ports:
      - "11333:11333" # this will allow you to query ollama as well, curl localhost:11333
    networks:
      - internal-network
    expose:
      - "11333" # expose port 11333 internally
    environment:
      # Configure local instance of ollama to run on :11333
      - OLLAMA_HOST=0.0.0.0:11333
    restart: unless-stopped
    # uncomment for GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [ gpu ]
  # jupyter:
  #   image: quay.io/jupyter/base-notebook
  #   # build:
  #   #   context: jupyter-workspace
  #   #   dockerfile: Dockerfile
  #   ports:
  #     - "11469:11469"
  #   networks:
  #     - internal-network
  #   volumes:
  #     - jupyter-data:/app
  #   environment:
  #     - PASSWORD=jupyter_test
  #     - JUPYTER_TOKEN=jupyter_test
  #     - JUPYTER_PORT=11469

  # Text To Speech Model (Kokoro)
  # docker run -d -p 8880:8880 -p 7860:7860 remsky/kokoro-fastapi:latest
  # kokoro-tts:
  #   image: remsky/kokoro-fastapi:latest
  #   container_name: kokoro-tts
  #   networks:
  #     - internal-network
  #   ports:
  #     - "8880:8880"
  #   restart: unless-stopped
  #   group_add:
  #     - "video"
  #     - "render"

networks:
  internal-network:
    driver: bridge

volumes:
  caddy-data:
  caddy-config:
  open-webui:
  ollama-docker:
  # jupyter-data:
